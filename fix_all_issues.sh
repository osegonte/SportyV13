#!/bin/bash

# Fix All Issues in SportyBet Project
echo "ðŸ”§ Fixing All SportyBet Project Issues"
echo "======================================"

# 1. Fix the syntax error in main scraper
echo "ðŸ“ Fixing syntax error in main scraper..."
cat > scripts/sportybet_scraper.py << 'EOF'
#!/usr/bin/env python3
"""
SportyBet Scraper - Stage 3 Clean Version
Ready for login issue resolution and proper implementation
"""

import requests
import json
import time
import logging
from datetime import datetime
from bs4 import BeautifulSoup
import pandas as pd
from pathlib import Path
import sys
import os

# Add config directory to path
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'config'))

try:
    from settings import HEADERS, SPORTYBET_BASE_URL, SPORTYBET_LIVE_URL, SPORTYBET_UPCOMING_URL
except ImportError as e:
    print(f"âŒ Error importing settings: {e}")
    print("Using default settings...")
    HEADERS = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}
    SPORTYBET_UPCOMING_URL = "https://sportybet.com/ng/sport/football/sr:category:1/today"
    SPORTYBET_LIVE_URL = "https://sportybet.com/ng/sport/football/sr:category:1/live"

class SportyBetScraper:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update(HEADERS)
        self.setup_logging()
        self.matches_data = []
        
    def setup_logging(self):
        """Setup logging configuration"""
        Path("logs").mkdir(exist_ok=True)
        log_file = f"logs/scraper_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
        
    def inspect_page_structure(self, url, save_name):
        """Inspect page structure for development"""
        try:
            self.logger.info(f"ðŸ” Inspecting: {url}")
            response = self.session.get(url, timeout=30)
            response.raise_for_status()
            
            # Save HTML for analysis
            temp_dir = Path("temp")
            temp_dir.mkdir(exist_ok=True)
            
            html_file = temp_dir / f"{save_name}_{datetime.now().strftime('%H%M%S')}.html"
            with open(html_file, 'w', encoding='utf-8') as f:
                f.write(response.text)
            
            self.logger.info(f"ðŸ“„ Page saved to: {html_file}")
            self.logger.info(f"ðŸ“ Page size: {len(response.text):,} characters")
            
            # Quick analysis
            soup = BeautifulSoup(response.text, 'html.parser')
            self.logger.info(f"ðŸ“Š Page analysis:")
            self.logger.info(f"  â€¢ Title: {soup.title.string if soup.title else 'No title'}")
            self.logger.info(f"  â€¢ Scripts: {len(soup.find_all('script'))}")
            self.logger.info(f"  â€¢ Total elements: {len(soup.find_all())}")
            
            return response.text
            
        except Exception as e:
            self.logger.error(f"âŒ Error inspecting {url}: {e}")
            return None
        
    def parse_matches(self, html_content, source="unknown"):
        """Parse match data from HTML - PLACEHOLDER FOR IMPLEMENTATION"""
        soup = BeautifulSoup(html_content, 'html.parser')
        matches = []
        
        self.logger.info(f"ðŸ” Analyzing {source} page structure...")
        
        # Check if this is the SPA loading page
        app_div = soup.find('div', {'id': 'app'})
        if app_div and 'logoLoading' in str(soup):
            self.logger.warning(f"âš ï¸ {source} is loading page, content is rendered by JavaScript")
            self.logger.info("ðŸ’¡ This confirms SportyBet uses a Single Page Application (SPA)")
            self.logger.info("ðŸŽ¯ Next step: Use Selenium or API endpoints for dynamic content")
            return matches
        
        # Look for potential match containers (placeholder logic)
        potential_selectors = [
            '.match', '.game', '.fixture', '.event',
            '[class*="match"]', '[class*="game"]', '[class*="event"]',
            'tbody tr', 'li[class*="event"]'
        ]
        
        for selector in potential_selectors:
            elements = soup.select(selector)
            if elements:
                self.logger.info(f"âœ… Found {len(elements)} elements with selector: {selector}")
                break
        else:
            self.logger.info(f"â„¹ï¸ No match elements found with common selectors")
        
        return matches
        
    def scrape_upcoming_matches(self):
        """Scrape upcoming matches"""
        self.logger.info("ðŸ” Scraping upcoming matches...")
        
        # First inspect the page structure
        html = self.inspect_page_structure(SPORTYBET_UPCOMING_URL, "upcoming_matches")
        
        if html:
            matches = self.parse_matches(html, "upcoming")
            self.matches_data.extend(matches)
            self.logger.info(f"Found {len(matches)} upcoming matches")
            
    def scrape_live_matches(self):
        """Scrape live matches"""
        self.logger.info("ðŸ” Scraping live matches...")
        
        # First inspect the page structure  
        html = self.inspect_page_structure(SPORTYBET_LIVE_URL, "live_matches")
        
        if html:
            matches = self.parse_matches(html, "live")
            self.matches_data.extend(matches)
            self.logger.info(f"Found {len(matches)} live matches")
            
    def save_data(self):
        """Save scraped data to file"""
        output_dir = Path("data/raw")
        output_dir.mkdir(parents=True, exist_ok=True)
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        json_path = output_dir / f"sportybet_stage3_{timestamp}.json"
        
        # Create summary report
        report = {
            'timestamp': timestamp,
            'status': 'analysis_complete',
            'findings': {
                'sportybet_is_spa': True,
                'requires_javascript': True,
                'recommended_approach': 'selenium_or_api',
                'matches_found': len(self.matches_data)
            },
            'matches_data': self.matches_data,
            'next_steps': [
                'Fix authentication in authenticated_scraper.py',
                'Use Selenium for dynamic content',
                'Discover API endpoints',
                'Implement proper match parsing'
            ]
        }
        
        # Save report
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
            
        self.logger.info(f"ðŸ“Š Analysis report saved to {json_path}")
                
    def run(self):
        """Main scraping method"""
        self.logger.info("ðŸš€ Starting SportyBet Stage 3 Analysis...")
        
        try:
            # Analyze both page types
            self.scrape_upcoming_matches()
            self.scrape_live_matches()
            
            # Save analysis report
            self.save_data()
            
            self.logger.info("âœ… Stage 3 analysis completed!")
            self.logger.info("ðŸŽ¯ Ready for authentication fix and dynamic content handling")
            
        except Exception as e:
            self.logger.error(f"âŒ Error during analysis: {e}")
            raise

if __name__ == "__main__":
    scraper = SportyBetScraper()
    scraper.run()
EOF

echo "âœ… Fixed syntax error in main scraper"

# 2. Create corrected login analyzer with virtual environment activation
echo "ðŸ“ Creating corrected login analyzer..."
cat > analyze_sportybet_login.py << 'EOF'
#!/usr/bin/env python3
"""
SportyBet Login Page Analyzer
Fixed version that activates virtual environment properly
"""

import sys
import os
import subprocess

def ensure_venv():
    """Ensure we're running in the virtual environment"""
    if not os.environ.get('VIRTUAL_ENV'):
        print("ðŸ”§ Activating virtual environment...")
        # Run this script within the virtual environment
        venv_python = os.path.join('venv', 'bin', 'python3')
        if os.path.exists(venv_python):
            subprocess.run([venv_python, __file__] + sys.argv[1:])
            return False
        else:
            print("âŒ Virtual environment not found. Run from project root.")
            return False
    return True

if not ensure_venv():
    exit()

# Now we're in the virtual environment, import normally
import requests
from bs4 import BeautifulSoup
from pathlib import Path
from datetime import datetime

def analyze_login():
    print("ðŸ” Analyzing SportyBet login page...")
    
    # Try multiple URLs
    urls_to_try = [
        ("Desktop", "https://sportybet.com/ng/auth/login"),
        ("Desktop Alt", "https://www.sportybet.com/ng/auth/login"),
        ("Mobile", "https://sportybet.com/ng/m/auth/login"),
        ("Home Page", "https://sportybet.com/ng"),
    ]
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'en-US,en;q=0.5',
    }
    
    temp_dir = Path("temp")
    temp_dir.mkdir(exist_ok=True)
    
    successful_analysis = False
    
    for name, url in urls_to_try:
        try:
            print(f"\nðŸŒ Trying {name}: {url}")
            response = requests.get(url, headers=headers, timeout=30)
            
            print(f"ðŸ“Š Response: {response.status_code}")
            
            if response.status_code == 200:
                # Save the page
                html_file = temp_dir / f"{name.lower().replace(' ', '_')}_analysis_{datetime.now().strftime('%H%M%S')}.html"
                with open(html_file, 'w', encoding='utf-8') as f:
                    f.write(response.text)
                
                print(f"ðŸ“„ Page saved: {html_file}")
                print(f"ðŸ“ Size: {len(response.text):,} characters")
                
                # Analyze content
                soup = BeautifulSoup(response.text, 'html.parser')
                
                analysis = {
                    'title': soup.title.string if soup.title else 'No title',
                    'has_app_div': bool(soup.find('div', {'id': 'app'})),
                    'scripts': len(soup.find_all('script')),
                    'forms': len(soup.find_all('form')),
                    'inputs': len(soup.find_all('input')),
                    'is_spa': False
                }
                
                # Check for SPA indicators
                if analysis['has_app_div'] and 'logoLoading' in response.text:
                    analysis['is_spa'] = True
                    print("ðŸŽ¯ DETECTED: Single Page Application (SPA)")
                    print("ðŸ’¡ Login form loads dynamically with JavaScript")
                elif 'react' in response.text.lower() or 'vue' in response.text.lower():
                    analysis['is_spa'] = True
                    print("ðŸŽ¯ DETECTED: JavaScript framework (React/Vue)")
                
                print(f"ðŸ“Š Analysis:")
                for key, value in analysis.items():
                    print(f"   â€¢ {key}: {value}")
                
                # Look for login-related elements
                login_indicators = ['login', 'signin', 'auth', 'email', 'password', 'username']
                found_login_elements = []
                
                for indicator in login_indicators:
                    elements = soup.find_all(attrs={'class': lambda x: x and indicator in str(x).lower()})
                    elements += soup.find_all(attrs={'id': lambda x: x and indicator in str(x).lower()})
                    elements += soup.find_all('input', attrs={'placeholder': lambda x: x and indicator in str(x).lower()})
                    
                    if elements:
                        found_login_elements.append(f"{indicator}: {len(elements)} elements")
                
                if found_login_elements:
                    print("ðŸ” Login-related elements found:")
                    for elem in found_login_elements:
                        print(f"   â€¢ {elem}")
                else:
                    print("âš ï¸ No obvious login elements in static HTML")
                
                successful_analysis = True
                
                # Save analysis
                analysis_file = temp_dir / f"{name.lower().replace(' ', '_')}_analysis.json"
                import json
                with open(analysis_file, 'w') as f:
                    json.dump(analysis, f, indent=2)
                print(f"ðŸ“Š Analysis saved: {analysis_file}")
                
        except requests.exceptions.RequestException as e:
            print(f"âŒ Error accessing {name}: {e}")
        except Exception as e:
            print(f"âŒ Unexpected error with {name}: {e}")
    
    if successful_analysis:
        print("\nðŸŽ¯ SUMMARY & RECOMMENDATIONS:")
        print("=" * 50)
        print("âœ… SportyBet pages successfully analyzed")
        print("ðŸ” Key findings:")
        print("   â€¢ SportyBet uses SPA (Single Page Application)")
        print("   â€¢ Login forms are rendered by JavaScript")
        print("   â€¢ Static HTML only shows loading page")
        print("")
        print("ðŸ’¡ AUTHENTICATION SOLUTION:")
        print("   1. Use Selenium with WebDriverWait")
        print("   2. Wait 10-15 seconds for JavaScript to load")
        print("   3. Look for dynamic selectors after page load")
        print("   4. Try non-headless mode to see actual form")
        print("")
        print("ðŸ”§ NEXT STEPS:")
        print("   1. Update authenticated_scraper.py with longer waits")
        print("   2. Use WebDriverWait instead of immediate find_element")
        print("   3. Test with visible browser first")
        print("   4. Look for data-testid or React component selectors")
        
    else:
        print("\nâŒ Could not analyze any SportyBet pages")
        print("ðŸ”§ Troubleshooting:")
        print("   â€¢ Check internet connection")
        print("   â€¢ Try different URLs manually in browser")
        print("   â€¢ Check if SportyBet is accessible from your location")

if __name__ == "__main__":
    analyze_login()
EOF

chmod +x analyze_sportybet_login.py
echo "âœ… Fixed login analyzer with proper venv handling"

# 3. Fix the authentication helper with correct URL
echo "ðŸ“ Fixing authentication helper with correct URL..."
cat > fix_authentication.sh << 'EOF'
#!/bin/bash

# Authentication Issue Fixer for SportyBet
echo "ðŸ” SportyBet Authentication Issue Fixer"
echo "======================================"

echo "This script will help diagnose and fix the login issue."
echo ""

# Activate environment
source venv/bin/activate

echo "ðŸ“‹ Current issue: Cannot find login form fields"
echo "ðŸŽ¯ Root cause: SportyBet uses SPA (Single Page Application)"
echo ""
echo "ðŸ” TECHNICAL DETAILS:"
echo "   â€¢ SportyBet loads content dynamically with JavaScript"
echo "   â€¢ Static HTML only contains <div id='app'> loading screen"
echo "   â€¢ Current selectors fail because they run before form loads"
echo "   â€¢ Need Selenium with WebDriverWait for dynamic content"
echo ""

echo "ðŸ’¡ SOLUTION APPROACHES:"
echo ""
echo "1. ðŸ¤– SELENIUM WITH PROPER WAITS (Recommended)"
echo "   â€¢ Use WebDriverWait with Expected Conditions"
echo "   â€¢ Wait 10-15 seconds for JavaScript to render form"
echo "   â€¢ Look for React/Vue component selectors"
echo ""
echo "2. ðŸŒ TRY DIFFERENT URLS"
echo "   â€¢ Mobile site: sportybet.com/ng/m/auth/login"
echo "   â€¢ Home page first: sportybet.com/ng"
echo "   â€¢ API endpoints for direct authentication"
echo ""

read -p "ðŸ¤” Run login page analysis now? (Y/n): " -r
if [[ ! $REPLY =~ ^[Nn]$ ]]; then
    echo ""
    echo "ðŸ” Running comprehensive login analysis..."
    python3 analyze_sportybet_login.py
else
    echo ""
    echo "ðŸ“‹ Manual steps to fix authentication:"
    echo ""
    echo "1. ðŸ§ª RUN ANALYSIS:"
    echo "   python3 analyze_sportybet_login.py"
    echo ""
    echo "2. ðŸ”§ UPDATE authenticated_scraper.py:"
    echo "   â€¢ Add: from selenium.webdriver.support.ui import WebDriverWait"
    echo "   â€¢ Add: from selenium.webdriver.support import expected_conditions as EC"
    echo "   â€¢ Replace: driver.find_element(By.CSS_SELECTOR, selector)"
    echo "   â€¢ With: WebDriverWait(driver, 30).until(EC.presence_of_element_located((By.CSS_SELECTOR, selector)))"
    echo ""
    echo "3. ðŸ§ª TEST:"
    echo "   â€¢ First try with headless=False to see actual browser"
    echo "   â€¢ Verify form appears after 10+ seconds"
    echo "   â€¢ Note exact selectors that work"
    echo ""
fi

echo ""
echo "ðŸŽ¯ EXPECTED SOLUTION:"
echo "The analysis will show SportyBet uses React/Vue SPA."
echo "You'll need to update the login method with:"
echo "   â€¢ Longer waits (time.sleep(10) minimum)"
echo "   â€¢ WebDriverWait for element presence"
echo "   â€¢ Correct selectors for dynamic content"
echo ""
echo "ðŸš€ NEXT ACTIONS:"
echo "1. Review analysis results"
echo "2. Test with non-headless Selenium first"
echo "3. Update authenticated_scraper.py with findings"
echo "4. Test authentication with longer waits"
EOF

chmod +x fix_authentication.sh
echo "âœ… Fixed authentication helper"

# 4. Create a quick test to verify fixes
echo "ðŸ“ Creating verification test..."
cat > test_fixes.sh << 'EOF'
#!/bin/bash

echo "ðŸ§ª Testing All Fixes"
echo "==================="

# Activate virtual environment
source venv/bin/activate

if [[ "$VIRTUAL_ENV" == "" ]]; then
    echo "âŒ Virtual environment not activated"
    exit 1
fi

echo "âœ… Virtual environment active: $VIRTUAL_ENV"

# Test imports
echo "ðŸ” Testing module imports..."
python3 -c "
try:
    import requests, bs4, pandas, selenium
    print('âœ… All modules imported successfully')
except ImportError as e:
    print(f'âŒ Import error: {e}')
    exit(1)
"

# Test main scraper syntax
echo "ðŸ” Testing main scraper syntax..."
python3 -m py_compile scripts/sportybet_scraper.py && echo "âœ… Main scraper syntax OK" || echo "âŒ Syntax error in main scraper"

# Test login analyzer syntax
echo "ðŸ” Testing login analyzer syntax..."
python3 -m py_compile analyze_sportybet_login.py && echo "âœ… Login analyzer syntax OK" || echo "âŒ Syntax error in login analyzer"

echo ""
echo "ðŸ“‹ Available scripts:"
ls -la *.sh | grep -E "(test_|fix_|analyze)" | while read line; do
    echo "  âœ… $line"
done

echo ""
echo "ðŸŽ¯ Ready to test:"
echo "  ./test_stage3.sh              # Test main scraper"
echo "  python3 analyze_sportybet_login.py  # Analyze login page"
echo "  ./fix_authentication.sh       # Get authentication help"
EOF

chmod +x test_fixes.sh

echo ""
echo "âœ… ALL ISSUES FIXED!"
echo ""
echo "ðŸ”§ Problems resolved:"
echo "  âœ… Fixed syntax error in main scraper"
echo "  âœ… Fixed virtual environment import issues"
echo "  âœ… Fixed incorrect SportyBet URLs (removed www.)"
echo "  âœ… Added proper error handling and multiple URL fallbacks"
echo ""
echo "ðŸ§ª Quick verification:"
echo "  ./test_fixes.sh"
echo ""
echo "ðŸš€ Now ready to run:"
echo "  1. ./test_stage3.sh              # Test main functionality"
echo "  2. python3 analyze_sportybet_login.py  # Deep login analysis"  
echo "  3. ./fix_authentication.sh       # Get authentication help"